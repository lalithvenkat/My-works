{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "invisible-district",
   "metadata": {},
   "source": [
    "#  Big Data Management Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessory-addition",
   "metadata": {},
   "source": [
    "We answer the same questions which we had answered in Assignment 1 using Data frame API. But in this Assignment 2 we'll answer these questions using Apache Spark's SQL API. We had defined Cassandra structures and the Spark code that\n",
    "saves the computed batch views into these structures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "other-mainstream",
   "metadata": {},
   "source": [
    "Importing all sparkcontext, sparkconf,sparksession from pyspark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "ae8454e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf \n",
    "from pyspark.sql import SparkSession\n",
    "sc = SparkContext.getOrCreate();\n",
    "spark=SparkSession.builder.getOrCreate()\n",
    "ss= spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "black-concept",
   "metadata": {},
   "source": [
    "### 1) Loading the csv file from the Hdfs file system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorrect-passport",
   "metadata": {},
   "source": [
    "Loading the csv file from hdfs requires the hadoop services to be started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "c33a35f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle_records_DF = spark.read.csv(\"hdfs://localhost:9000/user/lalith/data/per-vehicle-records-2021-01-31.csv\", inferSchema = True, header = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "3cd86f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+---+----+------+------+-----------+-----------+----+--------+------------+----------------+-----+---------+------+-------+----+-----+------+-----------+--------+------------+-------------+-----------+------------+\n",
      "|cosit|year|month|day|hour|minute|second|millisecond|minuteofday|lane|lanename|straddlelane|straddlelanename|class|classname|length|headway| gap|speed|weight|temperature|duration|validitycode|numberofaxles|axleweights|axlespacings|\n",
      "+-----+----+-----+---+----+------+------+-----------+-----------+----+--------+------------+----------------+-----+---------+------+-------+----+-----+------+-----------+--------+------------+-------------+-----------+------------+\n",
      "|  998|2021|    1| 31|   2|    45|     0|          0|        165|   2|    Ch 2|           0|            null|    2|      CAR|   5.2|   1.07|1.13| 71.0|   0.0|        0.0|       0|           0|            0|       null|        null|\n",
      "|  998|2021|    1| 31|   2|    45|     1|          0|        165|   2|    Ch 2|           0|            null|    5|  HGV_RIG|  11.1|    1.1|1.34| 69.0|   0.0|        0.0|       0|           0|            0|       null|        null|\n",
      "|  998|2021|    1| 31|   2|    45|     1|          0|        165|   1|    Ch 1|           0|            null|    5|  HGV_RIG|  11.1|   1.17|1.11| 69.0|   0.0|        0.0|       0|           0|            0|       null|        null|\n",
      "|  998|2021|    1| 31|   2|    45|     2|          0|        165|   1|    Ch 1|           0|            null|    2|      CAR|   5.3|    1.0|0.72| 70.0|   0.0|        0.0|       0|           0|            0|       null|        null|\n",
      "|  998|2021|    1| 31|   2|    45|     3|          0|        165|   2|    Ch 2|           0|            null|    3|      LGV|   5.3|   1.01|0.72| 71.0|   0.0|        0.0|       0|           0|            0|       null|        null|\n",
      "|  998|2021|    1| 31|   2|    45|     4|          0|        165|   1|    Ch 1|           0|            null|    2|      CAR|   5.2|   1.62|1.63| 70.0|   0.0|        0.0|       0|           0|            0|       null|        null|\n",
      "|  998|2021|    1| 31|   2|    45|     5|          0|        165|   2|    Ch 2|           0|            null|    3|      LGV|   5.2|   1.64|1.63| 69.0|   0.0|        0.0|       0|           0|            0|       null|        null|\n",
      "|  998|2021|    1| 31|   2|    45|     6|          0|        165|   1|    Ch 1|           0|            null|    5|  HGV_RIG|  11.4|   1.13|1.53| 70.0|   0.0|        0.0|       0|           0|            0|       null|        null|\n",
      "|  998|2021|    1| 31|   2|    45|     7|          0|        165|   2|    Ch 2|           0|            null|    5|  HGV_RIG|  11.4|   1.39|1.83| 71.0|   0.0|        0.0|       0|           0|            0|       null|        null|\n",
      "|  998|2021|    1| 31|   2|    45|     8|          0|        165|   1|    Ch 1|           0|            null|    5|  HGV_RIG|  11.1|   1.36|1.31| 69.0|   0.0|        0.0|       0|           0|            0|       null|        null|\n",
      "|  998|2021|    1| 31|   2|    45|     8|          0|        165|   2|    Ch 2|           0|            null|    2|      CAR|   5.2|   1.57|1.22| 69.0|   0.0|        0.0|       0|           0|            0|       null|        null|\n",
      "|  998|2021|    1| 31|   2|    45|     9|          0|        165|   1|    Ch 1|           0|            null|    2|      CAR|   5.2|   1.16|0.92| 70.0|   0.0|        0.0|       0|           0|            0|       null|        null|\n",
      "|  998|2021|    1| 31|   2|    45|    10|          0|        165|   2|    Ch 2|           0|            null|    5|  HGV_RIG|  11.5|   1.34|1.63| 71.0|   0.0|        0.0|       0|           0|            0|       null|        null|\n",
      "|  998|2021|    1| 31|   2|    45|    11|          0|        165|   1|    Ch 1|           0|            null|    5|  HGV_RIG|  11.1|   1.72|1.93| 69.0|   0.0|        0.0|       0|           0|            0|       null|        null|\n",
      "|  998|2021|    1| 31|   2|    45|    12|          0|        165|   2|    Ch 2|           0|            null|    2|      CAR|   5.2|   1.34|1.12| 71.0|   0.0|        0.0|       0|           0|            0|       null|        null|\n",
      "|  998|2021|    1| 31|   2|    45|    13|          0|        165|   1|    Ch 1|           0|            null|    2|      CAR|   5.1|   1.17|0.92| 69.0|   0.0|        0.0|       0|           0|            0|       null|        null|\n",
      "|  998|2021|    1| 31|   2|    45|    14|          0|        165|   1|    Ch 1|           0|            null|    2|      CAR|   5.1|   1.28|1.23| 69.0|   0.0|        0.0|       0|           0|            0|       null|        null|\n",
      "|  998|2021|    1| 31|   2|    45|    14|          0|        165|   2|    Ch 2|           0|            null|    5|  HGV_RIG|  11.3|   1.07|1.34| 69.0|   0.0|        0.0|       0|           0|            0|       null|        null|\n",
      "|  998|2021|    1| 31|   2|    45|    15|          0|        165|   1|    Ch 1|           0|            null|    2|      CAR|   5.1|   1.02|1.03| 69.0|   0.0|        0.0|       0|           0|            0|       null|        null|\n",
      "|  998|2021|    1| 31|   2|    45|    16|          0|        165|   2|    Ch 2|           0|            null|    5|  HGV_RIG|  11.5|   1.27|1.31| 71.0|   0.0|        0.0|       0|           0|            0|       null|        null|\n",
      "+-----+----+-----+---+----+------+------+-----------+-----------+----+--------+------------+----------------+-----+---------+------+-------+----+-----+------+-----------+--------+------------+-------------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vehicle_records_DF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "literary-lodging",
   "metadata": {},
   "source": [
    "Before performing the analysis on the Dataset, Intially we need to filter the dataset by identifying the M50 cosits from  junction 3 to junction 17 as per requirement.\n",
    "\n",
    "Firstly, we need to identify the cosits on m50 highway by refering to the map of irish road traffic data and the dataset.\n",
    "There are 15 counters present on m50 highway out of which 13 counters present in between junction 3 to 17. \n",
    "\n",
    "we need data for only those counters. so let's create a dataframe that contains only the required data by filtering the orginial dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "a9664fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cositlist = [1500,1501,1502,1508,1503,1509,1504,1505,1506,1507,15010,15011,15012]\n",
    "\n",
    "m50cositdf = vehicle_records_DF.where(vehicle_records_DF.cosit.isin(cositlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "07eedeed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(m50cositdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "positive-creator",
   "metadata": {},
   "source": [
    "Creating a temporary table out from the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "2af222d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "m50cositdf.registerTempTable(\"m50cositdf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acceptable-helping",
   "metadata": {},
   "source": [
    "#### 1) Calculating the usage of Irish road network in terms of percentage by vehicle category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "0794be23",
   "metadata": {},
   "outputs": [],
   "source": [
    "Irishroadusage = spark.sql(\"SELECT classname, COUNT(classname) AS count,\\\n",
    "round(count(classname)*100 / (select count(*) from m50cositdf),1) \\\n",
    "AS percentage from m50cositdf  GROUP BY classname ORDER BY percentage desc\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "d8108bfe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+----------+\n",
      "|classname| count|percentage|\n",
      "+---------+------+----------+\n",
      "|      CAR|154927|      85.3|\n",
      "|      LGV| 17860|       9.8|\n",
      "|  HGV_ART|  5460|       3.0|\n",
      "|  HGV_RIG|  1262|       0.7|\n",
      "|      BUS|  1107|       0.6|\n",
      "|  CARAVAN|   610|       0.3|\n",
      "|    MBIKE|   355|       0.2|\n",
      "|     null|     0|       0.0|\n",
      "+---------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Irishroadusage.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "particular-opportunity",
   "metadata": {},
   "source": [
    "From the above query we can find the usage of irish road network by each vehicle class. From the result we can observe that classname CAR has the highest uasge with percentage of 85.321"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "be61f010",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.readwriter.DataFrameWriter at 0x7fad7ce7b1d0>"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Irishroadusage.write.format(\"org.apache.spark.sql.cassandra\").options(table=\"roadusage2\", keyspace=\"assignment\")  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amber-volleyball",
   "metadata": {},
   "source": [
    "This line of code writes the result of the above spark sql code into the cassandra table called roadusage2 we had defined earlier. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offshore-balloon",
   "metadata": {},
   "source": [
    "#### 2) Calculating the highest and lowest hourly fows on M50 - show the hours and total number of vehicle counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "allied-shipping",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourlyflow = spark.sql(\"SELECT hour, count(hour) AS count from m50cositdf GROUP BY hour ORDER BY count desc \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "cathedral-special",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|hour|count|\n",
      "+----+-----+\n",
      "|  15|17211|\n",
      "|  16|16575|\n",
      "|  14|16373|\n",
      "|  13|15784|\n",
      "|  17|14742|\n",
      "|  12|11768|\n",
      "|  18|11503|\n",
      "|  11|10943|\n",
      "|  19| 9769|\n",
      "|  20| 9437|\n",
      "|  10| 8310|\n",
      "|  21| 6426|\n",
      "|   9| 6055|\n",
      "|   7| 5720|\n",
      "|   8| 4922|\n",
      "|   6| 3483|\n",
      "|  22| 3389|\n",
      "|  23| 2664|\n",
      "|   0| 1844|\n",
      "|   5| 1288|\n",
      "|   1| 1187|\n",
      "|   2|  891|\n",
      "|   4|  788|\n",
      "|   3|  510|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hourlyflow.show(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedicated-albany",
   "metadata": {},
   "source": [
    "The above query gives the count of vehicles per hour, we can observe that hours 13 -17 has highest uage, while hours 1 - 5 has least uasge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "disabled-combat",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourlyflow.write.format(\"org.apache.spark.sql.cassandra\").options(table=\"flow\", keyspace=\"assignment\").save(mode=\"append\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proof-crest",
   "metadata": {},
   "source": [
    "This line of code writes the result of the above spark sql code into the cassandra table called flow, which we had defined earlier. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interior-liberty",
   "metadata": {},
   "source": [
    "#### 3) Calculating the evening and morning rush hours on M50 - show the hours and the total counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "final-patent",
   "metadata": {},
   "outputs": [],
   "source": [
    "morning_hours = [9,10,11]\n",
    "evening_hours = [20,21,22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "textile-asset",
   "metadata": {},
   "outputs": [],
   "source": [
    "morninghours = spark.sql(\"SELECT hour, count(hour) AS count from m50cositdf WHERE hour IN (9,10,11)\\\n",
    "GROUP BY hour ORDER BY count desc \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "practical-trainer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|hour|count|\n",
      "+----+-----+\n",
      "|  11|10943|\n",
      "|  10| 8310|\n",
      "|   9| 6055|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "morninghours.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suitable-lending",
   "metadata": {},
   "source": [
    "The above query identifies the morning rush hours and the count of vehicles per that hour. We can observe that morning rush hours are between 9 -12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "combined-relation",
   "metadata": {},
   "outputs": [],
   "source": [
    "morninghours.write.format(\"org.apache.spark.sql.cassandra\").\\\n",
    "options(table=\"mrushhours\", keyspace=\"assignment\").save(mode=\"append\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organized-grocery",
   "metadata": {},
   "source": [
    "This line of code writes the result of the above spark sql code into the cassandra table called \"mrushhours\", which we had defined earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "wicked-motor",
   "metadata": {},
   "outputs": [],
   "source": [
    "eveninghours = spark.sql(\"SELECT hour, count(hour) AS count from m50cositdf WHERE hour IN (20,21,22)\\\n",
    "GROUP BY hour ORDER BY count desc \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "adjacent-reducing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|hour|count|\n",
      "+----+-----+\n",
      "|  20| 9437|\n",
      "|  21| 6426|\n",
      "|  22| 3389|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eveninghours.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moral-desperate",
   "metadata": {},
   "source": [
    "The above query identifies the Evening rush hours and the count of vehicles per that hour. We can observe that evening rush hours are between 20 -22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "acting-integer",
   "metadata": {},
   "outputs": [],
   "source": [
    "eveninghours.write.format(\"org.apache.spark.sql.cassandra\").\\\n",
    "options(table=\"erushhours\", keyspace=\"assignment\").save(mode=\"append\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educated-image",
   "metadata": {},
   "source": [
    "This line of code writes the result of the above spark sql code into the cassandra table called \"erushhours\", which we had defined earlier. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trained-finish",
   "metadata": {},
   "source": [
    "#### 4) Calculating the average speed between each junctions on M50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "second-membership",
   "metadata": {},
   "outputs": [],
   "source": [
    "averagespeed = spark.sql(\"SELECT cosit, round(AVG(speed),2) AS avgspeed from m50cositdf GROUP BY cosit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "stylish-penguin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "|cosit|avgspeed|\n",
      "+-----+--------+\n",
      "| 1507|  102.74|\n",
      "| 1500|   93.75|\n",
      "| 1506|  102.79|\n",
      "| 1505|   99.69|\n",
      "| 1504|  101.99|\n",
      "|15010|  105.02|\n",
      "|15012|   105.1|\n",
      "| 1509|   98.35|\n",
      "| 1502|  102.36|\n",
      "|15011|   101.8|\n",
      "| 1501|  101.33|\n",
      "| 1503|  102.18|\n",
      "| 1508|   98.65|\n",
      "+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "averagespeed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advisory-virginia",
   "metadata": {},
   "source": [
    "The above the query gives the average vehicle speed recorded at each counter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "headed-plymouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "averagespeed.write.format(\"org.apache.spark.sql.cassandra\").\\\n",
    "options(table=\"avg_speed\", keyspace=\"assignment\").save(mode=\"append\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wanted-reform",
   "metadata": {},
   "source": [
    "This line of code writes the result of the above spark sql code into the cassandra table called \"avg_speed\", which we had defined earlier. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anonymous-timber",
   "metadata": {},
   "source": [
    "Here i have also added junction names of the each cosit to the result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "short-screw",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|      junction|cosit|\n",
      "+--------------+-----+\n",
      "|  Junction 3-4| 1500|\n",
      "|  Junction 4-5| 1501|\n",
      "|  Junction 5-6| 1502|\n",
      "|  Junction 6-7| 1508|\n",
      "|  Junction 7-9| 1503|\n",
      "| Junction 9-10| 1509|\n",
      "|Junction 10-11| 1504|\n",
      "|Junction 11-12| 1505|\n",
      "|Junction 12-13| 1506|\n",
      "|Junction 13-14| 1507|\n",
      "|Junction 14-15|15010|\n",
      "|Junction 15-16|15011|\n",
      "|Junction 16-17|15012|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "junctionlist =[(\"Junction 3-4\", 1500),(\"Junction 4-5\", 1501),(\"Junction 5-6\", 1502),(\"Junction 6-7\",1508),(\"Junction 7-9\",1503),(\"Junction 9-10\",1509),(\"Junction 10-11\",1504),(\"Junction 11-12\",1505),(\"Junction 12-13\",1506),(\"Junction 13-14\",1507),(\"Junction 14-15\",15010),(\"Junction 15-16\",15011),(\"Junction 16-17\",15012)]\n",
    "jun= sc.parallelize(junctionlist).collect()\n",
    "juncdataf=spark.createDataFrame(jun, [\"junction\",\"cosit\"])\n",
    "juncdataf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "dynamic-australian",
   "metadata": {},
   "outputs": [],
   "source": [
    "juncdataf.registerTempTable(\"juncdataf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "national-course",
   "metadata": {},
   "outputs": [],
   "source": [
    "avgspeedbetweenjunc= spark.sql(\"SELECT m50cositdf.cosit,round(AVG(m50cositdf.speed),1) AS avgspeed,\\\n",
    "juncdataf.junction from m50cositdf JOIN juncdataf ON m50cositdf.cosit = juncdataf.cosit \\\n",
    "GROUP BY m50cositdf.cosit,juncdataf.junction \");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "variable-humor",
   "metadata": {},
   "outputs": [],
   "source": [
    "avgspeedbetweenjunc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "featured-frank",
   "metadata": {},
   "source": [
    "This is a refined output for this question, I have added the  junction numbers at which the respective counter is located and the percentage is rounded to 1 decimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absolute-brunei",
   "metadata": {},
   "outputs": [],
   "source": [
    "avgspeedbetweenjunc.write.format(\"org.apache.spark.sql.cassandra\").\\\n",
    "options(table=\"avgspeedatjunction\", keyspace=\"assignment\").save(mode=\"append\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anticipated-signal",
   "metadata": {},
   "source": [
    "This line of code writes the result of the above spark sql code into the cassandra table called \"avg_speed\", which we had defined earlier. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handed-weight",
   "metadata": {},
   "source": [
    "#### 5) Calculating the top 10 locations with highest number of counts of HGVs(class)and Mapping  the COSITs with their names given on the map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resident-basis",
   "metadata": {},
   "outputs": [],
   "source": [
    "locationlist =[(\"TMU M50 001.7 N\", 1500),(\"TMU M50 005.0 N\", 1501),(\"TMU M50 010.0 N\", 1502),(\"TMU M50 015.0 S\",1508),(\"TMU M50 020.0 N\",1503),(\"TMU M50 015.0 N\",1509),(\"TMU M50 025.0 S\",1504),(\"TMU M50 025.0 N\",1505),(\"TMU M50 030.0 S\",1506),(\"TMU M50 035.0 S\",1507),(\"TMU M50 040.0 S\",15010),(\"TMU M50 035.0 N\",15011),(\"TMU M50 040.0 N\",15012)]\n",
    "nm= sc.parallelize(locationlist).collect()\n",
    "dataf=spark.createDataFrame(nm, [\"location\",\"cosit\"])\n",
    "dataf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cognitive-appliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataf.registerTempTable(\"dataf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statistical-printer",
   "metadata": {},
   "source": [
    "We have created a new samll dataframe called dataf, It contains the names of the location for each cosit. This dataframe has been converted to temporary table for running the sql queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced-jacob",
   "metadata": {},
   "outputs": [],
   "source": [
    "toplocation = spark.sql(\"SELECT  Distinct m50cositdf.cosit,  count(m50cositdf.cosit) AS count,\\\n",
    "dataf.location from m50cositdf JOIN dataf ON m50cositdf.cosit = dataf.cosit \\\n",
    "WHERE m50cositdf.classname ='HGV_RIG' OR m50cositdf.classname = 'HGV_ART'  \\\n",
    "GROUP BY m50cositdf.cosit, dataf.location ORDER BY count desc\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "micro-antenna",
   "metadata": {},
   "outputs": [],
   "source": [
    "toplocation.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "champion-mainland",
   "metadata": {},
   "source": [
    "We have joined the two temporary tables to the get the location name next to the cosit, The above query gives the top 10 locations and its cosits with highest counts of vehicles class \"HGV\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dimensional-inspector",
   "metadata": {},
   "outputs": [],
   "source": [
    "toplocation.write.format(\"org.apache.spark.sql.cassandra\").\\\n",
    "options(table=\"toplocations\", keyspace=\"assignment\").save(mode=\"append\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reduced-nepal",
   "metadata": {},
   "source": [
    "This line of code writes the result of the above spark sql code into the cassandra table called \"toplocations\", which we had defined earlier. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
